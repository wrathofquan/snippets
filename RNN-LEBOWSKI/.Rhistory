html_nodes('.figure-table tr') %>%
html_table()
dates <- gallup %>%
html_nodes('.figure-table tr') %>%
html_table()
dates <- gallup %>%
html_nodes('.figure-table tr')
dates
str(dates)
dates <- gallup %>%
html_nodes('.figure-table tr')
dates <- gallup %>%
html_nodes('.figure-table tr') %>%
html_text()
dates
library(tidyverse)
getwd()
setwd("/Users/joshuaquan/Downloads/dlab_dBrown")
ls()
dir()
errorRatings <- read.csv("MSpilot_ErrorRatings.csv")
head(errorRatings)
dir()
qualtricsRaw <- read.csv("DivMoral_S1_dlabHelp.csv")
View(errorRatings)
library(tidyverse)
errorRatings %>%
rally()
errorRatings %>%
tally()
errorRatings
errorRatings %>%
count(D1pay_NT)
head(errorRatings)
errorRatings %>%
count(D1pay_NT,D1pay_S)
errorRatings %>%
count(D1pay_NT, D1pay_S) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT, D1pay_S) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT, D1pay_S) %>%
arrange() %>%
qplot()
errorRatings %>%
na.omit() %>%
count(D1pay_NT, D1pay_S) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT))
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_hist()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_bar()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = count(D1pay_NT))) + geom_bar()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
hist()
errorRatings %>%
as.numeric() %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
# 2. DivMoral_S1_dlabHelp
# I want to use this data set mostly to learn how to clean a raw data file from Qualtrics. This was a between-subjects experiment and I would like to make new variables based on the condition the participant was exposed to and based on the specific manipulation. These new variables will be my IVs for t-test / (M)ANOVA analysis.
# Generally, I'd like to develop skills on:
# transforming wide --> long, and vice versa, data sets (we wouldn't need to do this for this data set specifically, but could use the MSpilot data for practice)
# creating new variables based on condition
# creating and saving a new "clean" data file (separate from the Raw data file)
# Creating composite variables
# Recoding variables
qualtricsRaw <- read.csv("DivMoral_S1_dlabHelp.csv")
errorRatings <- read.csv("MSpilot_ErrorRatings.csv", stringsAsFactors = F)
errorRatings %>%
as.numeric() %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
hist()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange()
102.4 + 227.55
329.95 *1.30
329.95 * .
329.95 * .3
329.95 * (329.96 * .3)
329.95 - (329.96 * .3)
qualtricsRaw <- read.csv("DivMoral_S1_dlabHelp.csv")
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n, y - D1pay_NT)) + geom_bar()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n, y = D1pay_NT)) + geom_bar()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n, y = D1pay_NT)) + geom_bar(stat = identity)
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n, y = D1pay_NT))
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n, y = D1pay_NT)) + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(y = D1pay_NT)) + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram(stat= count)
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT), stat = count) + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT), stat = 'count') + geom_histogram()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange()
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_bar()
?identity
?stat
??stat_identity
errorRatings %>%
na.omit() %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_bar() + stat_identity()
errorRatings %>%
count(D1pay_NT) %>%
arrange() %>%
qplot(aes(x = D1pay_NT, y = n))
errorRatings %>%
count(D1pay_NT) %>%
arrange() %>%
qplot(x = D1pay_NT, y = n)
errorRatings %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_point()
errorRatings %>%
filter
count(D1pay_NT) %>%
arrange()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_point()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_area()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_area()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_count()
?geom_count
head(qualtricsRaw)
View(qualtricsRaw)
sumary(qualtricsRaw)
summary(qualtricsRaw)
library(tidyverse)
errorRatings <- read.csv("MSpilot_ErrorRatings.csv", stringsAsFactors = F)
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_count()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_line()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_line()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_point()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
hist()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
hist(x = D1pay_NT)
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
hist(x = D1pay_NT)
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot() + geom_histogram()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT), stat="count") + geom_histogram()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT), stat="count") + geom_histogram(stat="count")
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram(stat="count")
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = n)) + geom_histogram(stat="count")
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram(stat="count")
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_histogram(stat="identity")
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT)) + geom_bar()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_bar()
errorRatings %>%
filter(D1pay_NT != "") %>%
count(D1pay_NT) %>%
arrange() %>%
ggplot(aes(x = D1pay_NT, y = n)) + geom_bar(stat = "identity")
View(errorRatings)
library(tokenizers)
install.packages(c('keras', 'tokenizers'))
getwd()
setwd("~/Downloads/RNN-LEBOWSKI")
ls()
dir()
install.packages('janeaustenr')
library(janeaustenr)
text <- austen_books() %>%
filter(book == "Pride & Prejudice") %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
library(tokenizers)
text <- austen_books() %>%
filter(book == "Pride & Prejudice") %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("Corpus length: %d", length(text)))
dir()
leb <- read.tsv("biglebowski.txt")
library(tidyverse)
leb <- read.tsv("biglebowski.txt")
leb <- read_tsv("biglebowski.txt")
View(leb)
summary(leb)
text <- leb %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
install.packages("textgenrnn")
library(textgenrnn)
library(textgenrnn)
install.packages('rnn')
library(rnn)
tbl <- list.files(pattern = "*.txt") %>%
map_chr(~ read_file(.)) %>%
data_frame(text = .)
View(tbl)
tbl
print(tbl)
text <- tbl %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
text
print(sprintf("Corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("Total characters: %d", length(chars)))
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
dataset <- transpose(dataset)
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
library(keras)
library(tidyverse)
library(tokenizers)
library(textgenrnn)
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
max_length <- 40
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
dataset <- transpose(dataset)
View(dataset)
vectors <- vectorize(dataset, chars, max_length)
vectorize <- function(data, chars, max_length){
x <- array(0, dim = c(length(data$sentence), max_length, length(chars)))
y <- array(0, dim = c(length(data$sentence), length(chars)))
for(i in 1:length(data$sentence)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == data$sentence[[i]])
})
y[i,] <- as.integer(chars == data$next_char[[i]])
}
list(y = y,
x = x)
}
vectors <- vectorize(dataset, chars, max_length)
create_model <- function(chars, max_length){
keras_model_sequential() %>%
layer_lstm(128, input_shape = c(max_length, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax") %>%
compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
}
fit_model <- function(model, vectors, epochs = 1){
model %>% fit(
vectors$x, vectors$y,
batch_size = 128,
epochs = epochs
)
NULL
}
iterate_model <- function(model, text, chars, max_length,
diversity, vectors, iterations){
for(iteration in 1:iterations){
message(sprintf("iteration: %02d ---------------\n\n", iteration))
fit_model(model, vectors)
for(diversity in c(0.2, 0.5, 1)){
message(sprintf("diversity: %f ---------------\n\n", diversity))
current_phrase <- 1:10 %>%
map_chr(function(x) generate_phrase(model,
text,
chars,
max_length,
diversity))
message(current_phrase, sep="\n")
message("\n\n")
}
}
NULL
}
model <- create_model(chars, max_length)
iterate_model(model, text, chars, max_length, diversity, vectors, 40)
model <- create_model(chars, max_length)
iterate_model(model, text, chars, max_length, diversity, vectors, 40)
iterate_model <- function(model, text, chars, max_length,
diversity, vectors, iterations){
for(iteration in 1:iterations){
message(sprintf("iteration: %02d ---------------\n\n", iteration))
fit_model(model, vectors)
for(diversity in c(0.2, 0.5, 1)){
message(sprintf("diversity: %f ---------------\n\n", diversity))
current_phrase <- 1:10 %>%
map_chr(function(x) generate_phrase(model,
text,
chars,
max_length,
diversity))
message(current_phrase, sep="\n")
message("\n\n")
}
}
NULL
}
model <- create_model(chars, max_length)
iterate_model(model, text, chars, max_length, diversity, vectors, 40)
library(keras)
library(keras)
model <- create_model(chars, max_length)
iterate_model <- function(model, text, chars, max_length,
diversity, vectors, iterations){
for(iteration in 1:iterations){
message(sprintf("iteration: %02d ---------------\n\n", iteration))
fit_model(model, vectors)
for(diversity in c(0.2, 0.5, 1)){
message(sprintf("diversity: %f ---------------\n\n", diversity))
current_phrase <- 1:10 %>%
map_chr(function(x) generate_phrase(model,
text,
chars,
max_length,
diversity))
message(current_phrase, sep="\n")
message("\n\n")
}
}
NULL
}
model <- create_model(chars, max_length)
model <- create_model(chars, max_length)
